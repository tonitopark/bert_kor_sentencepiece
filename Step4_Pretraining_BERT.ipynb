{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain Bert with Sentencepiece model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refer to : \n",
    "\n",
    "### Estimator API\n",
    "https://www.tensorflow.org/guide/estimators\n",
    "\n",
    "### TPUEstimator API\n",
    "https://cloud.google.com/tpu/docs/tutorials/migrating-to-tpuestimator-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from bert import modeling\n",
    "from bert import optimization\n",
    "import tempfile\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/wiki-kor/AA/all-maxseq512.tfrecord', 'data/wiki-kor/AB/all-maxseq512.tfrecord', 'data/wiki-kor/AC/all-maxseq512.tfrecord', 'data/wiki-kor/AD/all-maxseq512.tfrecord', 'data/wiki-kor/AE/all-maxseq512.tfrecord', 'data/wiki-kor/AF/all-maxseq512.tfrecord', 'data/wiki-kor/AG/all-maxseq512.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "### Configuration paramters\n",
    "\n",
    "output_dir= 'model_test'\n",
    "\n",
    "train_batch_size=6\n",
    "max_seq_length=512\n",
    "max_predictions_per_seq=20\n",
    "num_train_steps=2000\n",
    "num_warmup_steps=1000\n",
    "save_checkpoints_steps=2000\n",
    "learning_rate=1e-4\n",
    "max_eval_steps=100\n",
    "\n",
    "tf_record_files=\"\"\"data/wiki-kor/AA/all-maxseq512.tfrecord,\n",
    "data/wiki-kor/AB/all-maxseq512.tfrecord,\n",
    "data/wiki-kor/AC/all-maxseq512.tfrecord,\n",
    "data/wiki-kor/AD/all-maxseq512.tfrecord,\n",
    "data/wiki-kor/AE/all-maxseq512.tfrecord,\n",
    "data/wiki-kor/AF/all-maxseq512.tfrecord,\n",
    "data/wiki-kor/AG/all-maxseq512.tfrecord\"\"\"\n",
    "\n",
    "input_files =[] \n",
    "for input_pattern in tf_record_files.split(\",\"):\n",
    "    input_files.extend(tf.gfile.Glob(input_pattern.strip()))\n",
    "print(input_files)\n",
    "\n",
    "\n",
    "bert_config =  modeling.BertConfig.from_json_file('model/bert_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Write a model function\n",
    "\n",
    "   * The model function used here has the following call signature\n",
    "   ```python\n",
    "   def my_model_fn(\n",
    "       features, # This is batch_features from input_fn\n",
    "       labels,   # This is batch_labels from input_fn\n",
    "       mode,     # An instance of tf.estimator.ModeKeys\n",
    "       params):  # Additional configuration\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1  Masked LM Task output function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
    "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,)\n",
    "INFO:tensorflow:  name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
    "INFO:tensorflow:  name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
    "INFO:tensorflow:  name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
    "INFO:tensorflow:  name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
    "INFO:tensorflow:  name = cls/predictions/output_bias:0, shape = (32000,)\n",
    "INFO:tensorflow:  name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
    "INFO:tensorflow:  name = cls/seq_relationship/output_bias:0, shape = (2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_lm_output(bert_config, input_tensor, output_weights,\n",
    "                         positions,label_ids, label_weights):\n",
    "    \n",
    "    def gather_indexes(input_tensor, positions):\n",
    "        # Gathers the vectors at the specific positions over a minibatch.\n",
    "        sequence_shape = modeling.get_shape_list(input_tensor, expected_rank=3)\n",
    "        batch_size = sequence_shape[0]\n",
    "        seq_length = sequence_shape[1]\n",
    "        width      = sequence_shape[2]\n",
    "\n",
    "        flat_offsets = tf.reshape(\n",
    "            tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
    "        flat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
    "        flat_sequence_tensor = tf.reshape(input_tensor,\n",
    "                                        [batch_size * seq_length, width])\n",
    "        output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n",
    "        return output_tensor\n",
    "    \n",
    "    \n",
    "    #Compute loss& log probs for the masked LM\n",
    "    \n",
    "    input_tensor = gather_indexes(input_tensor, positions)\n",
    "\n",
    "    with tf.variable_scope(\"cls/predictions\"):\n",
    "        # We apply one more non-linear transformation before the output layer.\n",
    "        # This matrix is not used after pre-training.\n",
    "        with tf.variable_scope(\"transform\"):\n",
    "            input_tensor = tf.layers.dense(\n",
    "                  input_tensor,\n",
    "                  units=bert_config.hidden_size,\n",
    "                  activation=modeling.get_activation(bert_config.hidden_act),\n",
    "                  kernel_initializer=modeling.create_initializer(\n",
    "                      bert_config.initializer_range))\n",
    "                \n",
    "            input_tensor = modeling.layer_norm(input_tensor)\n",
    "\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        output_bias = tf.get_variable(\n",
    "            \"output_bias\",\n",
    "            shape=[bert_config.vocab_size],\n",
    "            initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        label_ids = tf.reshape(label_ids, [-1])\n",
    "        label_weights = tf.reshape(label_weights, [-1])\n",
    "\n",
    "        one_hot_labels = tf.one_hot(\n",
    "            label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n",
    "\n",
    "        # The `positions` tensor might be zero-padded (if the sequence is too\n",
    "        # short to have the maximum number of predictions). The `label_weights`\n",
    "        # tensor has a value of 1.0 for every real prediction and 0.0 for the\n",
    "        # padding predictions.\n",
    "        per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n",
    "        numerator = tf.reduce_sum(label_weights * per_example_loss)\n",
    "        denominator = tf.reduce_sum(label_weights) + 1e-5\n",
    "        loss = numerator / denominator\n",
    "\n",
    "    return (loss, per_example_loss, log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2  Next Sentence Task output function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_sentence_output(bert_config, input_tensor, labels):\n",
    "    #Get loss and log probs for the next sentence prediction.\n",
    "\n",
    "    # Simple binary classification. \n",
    "    # 0 is \"next sentence\" \n",
    "    # 1 is \"random sentence\". \n",
    "    # This weight matrix is not used after pre-training.\n",
    "    with tf.variable_scope(\"cls/seq_relationship\"):\n",
    "        output_weights = tf.get_variable(\n",
    "            \"output_weights\",\n",
    "            shape=[2, bert_config.hidden_size],\n",
    "            initializer=modeling.create_initializer(bert_config.initializer_range))\n",
    "        output_bias = tf.get_variable(\n",
    "            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n",
    "\n",
    "        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        labels = tf.reshape(labels, [-1])\n",
    "        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, per_example_loss, log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3. Metrics function for computing loss and accuracy of evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\n",
    "              masked_lm_weights, next_sentence_example_loss,\n",
    "              next_sentence_log_probs, next_sentence_labels):\n",
    "\n",
    "    # Computes the loss and accuracy of the model\n",
    "    masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\n",
    "                                     [-1, masked_lm_log_probs.shape[-1]])\n",
    "    masked_lm_predictions = tf.argmax(\n",
    "        masked_lm_log_probs, axis=-1, output_type=tf.int32)\n",
    "    masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\n",
    "    masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\n",
    "    masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\n",
    "    masked_lm_accuracy = tf.metrics.accuracy(\n",
    "        labels=masked_lm_ids,\n",
    "        predictions=masked_lm_predictions,\n",
    "        weights=masked_lm_weights)\n",
    "    masked_lm_mean_loss = tf.metrics.mean(\n",
    "        values=masked_lm_example_loss, weights=masked_lm_weights)\n",
    "\n",
    "    next_sentence_log_probs = tf.reshape(\n",
    "        next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\n",
    "    next_sentence_predictions = tf.argmax(\n",
    "        next_sentence_log_probs, axis=-1, output_type=tf.int32)\n",
    "    next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\n",
    "    next_sentence_accuracy = tf.metrics.accuracy(\n",
    "        labels=next_sentence_labels, predictions=next_sentence_predictions)\n",
    "    next_sentence_mean_loss = tf.metrics.mean(\n",
    "        values=next_sentence_example_loss)\n",
    "\n",
    "    return {\n",
    "        \"masked_lm_accuracy\": masked_lm_accuracy,\n",
    "        \"masked_lm_loss\": masked_lm_mean_loss,\n",
    "        \"next_sentence_accuracy\": next_sentence_accuracy,\n",
    "        \"next_sentence_loss\": next_sentence_mean_loss,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.  Implementing Model function Builder for Estimator\n",
    "\n",
    "The tf.estimator.EstimatorSpec returned for evaluation typically contains the following information:\n",
    "\n",
    "   * loss, which is the model's loss\n",
    "   * eval_metric_ops, which is an optional dictionary of metrics.\n",
    "\n",
    "So, we'll create a dictionary containing our sole metric. If we had calculated other metrics, we would have added them as additional key/value pairs to that same dictionary. Then, we'll pass that dictionary in the eval_metric_ops argument of tf.estimator.EstimatorSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(bert_config, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps):\n",
    "    \n",
    "    # Return model_fn for Estimator\n",
    "\n",
    "    def model_fn(features, labels, mode, params):\n",
    "        \n",
    "        # Define the model_fn\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        masked_lm_positions = features[\"masked_lm_positions\"]\n",
    "        masked_lm_ids = features[\"masked_lm_ids\"]\n",
    "        masked_lm_weights = features[\"masked_lm_weights\"]\n",
    "        next_sentence_labels = features[\"next_sentence_labels\"]\n",
    "\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "        model = modeling.BertModel(\n",
    "            config=bert_config,\n",
    "            is_training=is_training,\n",
    "            input_ids=input_ids,\n",
    "            input_mask=input_mask,\n",
    "            token_type_ids=segment_ids,\n",
    "            use_one_hot_embeddings=False)\n",
    "\n",
    "        (masked_lm_loss,\n",
    "         masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(\n",
    "            bert_config, model.get_sequence_output(), model.get_embedding_table(),\n",
    "            masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    "\n",
    "        (next_sentence_loss, next_sentence_example_loss,\n",
    "         next_sentence_log_probs) = get_next_sentence_output(\n",
    "            bert_config, model.get_pooled_output(), next_sentence_labels)\n",
    "\n",
    "        total_loss = masked_lm_loss + next_sentence_loss\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "\n",
    "        initialized_variable_names = {}\n",
    "        \n",
    "        if init_checkpoint:\n",
    "            (assignment_map, initialized_variable_names\n",
    "             ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "\n",
    "        output_spec = None\n",
    "        \n",
    "        # Training\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            train_op = optimization.create_optimizer(\n",
    "                total_loss, learning_rate, num_train_steps, \n",
    "                num_warmup_steps,False)\n",
    "\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                train_op=train_op)\n",
    "        \n",
    "        # evaluation    \n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                eval_metric_ops = metric_fn(masked_lm_example_loss, \n",
    "                          masked_lm_log_probs, masked_lm_ids,\n",
    "                          masked_lm_weights, next_sentence_example_loss,\n",
    "                          next_sentence_log_probs, next_sentence_labels))\n",
    "        return output_spec\n",
    "\n",
    "    return model_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Function Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "    bert_config=bert_config,\n",
    "    init_checkpoint=None,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_steps = num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=output_dir,\n",
    "    save_checkpoints_steps=save_checkpoints_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'model_test', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5dc13a4470>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "params = {\"batch_size\":train_batch_size,\n",
    "         \"max_seq_length\":max_seq_length,\n",
    "         \"max_predictions_per_seq\":max_predictions_per_seq,\n",
    "         \"num_train_steps\":num_train_steps,\n",
    "         \"num_warmup_steps\":num_warmup_steps}\n",
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    config = run_config,\n",
    "    params=params\n",
    "    #train_batch_size=train_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Write an Input function\n",
    "\n",
    "  The input function will read records from .tfrecords  file,\n",
    "  parse each record and form a batch to be used with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_builder(input_files,\n",
    "                     max_seq_length,\n",
    "                     max_predictions_per_seq,\n",
    "                     is_training,\n",
    "                     num_cpu_threads=4):\n",
    "    #Creates an `input_fn` closure to be passed to TPUEstimator.\n",
    "\n",
    "    def input_fn(params):\n",
    "        #The actual input function\n",
    "        batch_size = params[\"batch_size\"]\n",
    "\n",
    "        d = tf.data.TFRecordDataset(input_files)\n",
    "\n",
    "        d = d.repeat()\n",
    "        \n",
    "        d = d.map(parse)\n",
    "        \n",
    "        d = d.batch(batch_size)\n",
    "        \n",
    "        return d\n",
    "\n",
    "    return input_fn\n",
    "\n",
    "\n",
    "def parse(record):\n",
    "    \n",
    "    name_to_features = {\n",
    "            \"input_ids\":\n",
    "                tf.FixedLenFeature([max_seq_length], tf.int64),\n",
    "            \"input_mask\":\n",
    "                tf.FixedLenFeature([max_seq_length], tf.int64),\n",
    "            \"segment_ids\":\n",
    "                tf.FixedLenFeature([max_seq_length], tf.int64),\n",
    "            \"masked_lm_positions\":\n",
    "                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "            \"masked_lm_ids\":\n",
    "                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),\n",
    "            \"masked_lm_weights\":\n",
    "                tf.FixedLenFeature([max_predictions_per_seq], tf.float32),\n",
    "            \"next_sentence_labels\":\n",
    "                tf.FixedLenFeature([1], tf.int64),\n",
    "        }\n",
    "\n",
    "    example = tf.parse_single_example(record, name_to_features)\n",
    "\n",
    "    for name in list(example.keys()):\n",
    "        t = example[name]\n",
    "        if t.dtype == tf.int64:\n",
    "            t = tf.to_int32(t)\n",
    "        example[name] = t\n",
    "\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Input Function Build for Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = input_fn_builder(\n",
    "    input_files=input_files,\n",
    "    max_seq_length=max_seq_length,\n",
    "    max_predictions_per_seq=max_predictions_per_seq,\n",
    "    is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training using estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping training since max_steps has already saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f5dc13a4dd8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Input Function Build for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_fn = input_fn_builder(\n",
    "    input_files=input_files,\n",
    "    max_seq_length=max_seq_length,\n",
    "    max_predictions_per_seq=max_predictions_per_seq,\n",
    "    is_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluation using estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-08-06:54:24\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from model_test/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-08-06:54:40\n",
      "INFO:tensorflow:Saving dict for global step 2000: global_step = 2000, loss = 8.713631, masked_lm_accuracy = 0.054181494, masked_lm_loss = 8.11758, next_sentence_accuracy = 0.71166664, next_sentence_loss = 0.5982522\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: model_test/model.ckpt-2000\n"
     ]
    }
   ],
   "source": [
    "result = estimator.evaluate(\n",
    "    input_fn=eval_input_fn, steps=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
